---
title: "Text_Mining_Assignment"
format: html
editor: visual
---

## Libraries

```{r}
library(readr)
library(tidyverse)
library(tidytext)
library(topicmodels)
library(tm)
library(quanteda)
library(widyr)


```

We analyze song lyrics from 1950-2020 to see if themes have changed.

```{r}
df <- read_csv("tcc_ceds_music.csv")
```

```{r}
head(df)

colnames(df)


df |>  duplicated() |>  sum()

df |>  summarise_all(~sum(is.na(.)))
```

```{r}

df |> 
  count(release_date) |> 
  ggplot(aes(x = release_date, y = n)) +
  geom_col(fill = "steelblue") +
  labs(title = "Number of Songs per Year",
       x = "Year", y = "Number of Songs") +
  theme_minimal()
```

```{r}

df |> 
  count(artist_name, sort = TRUE) |> 
  top_n(10) |> 
  ggplot(aes(x = reorder(artist_name, n), y = n)) +
  geom_col(fill = "tomato") +
  coord_flip() +
  labs(title = "Top 10 Artists",
       x = NULL, y = "Number of Songs") +
  theme_minimal()
```

```{r}

df <- df |> 
  mutate(decade = floor(release_date / 10) * 10) 


df |> 
  select(release_date, decade) %>%
  sample_n(5)


df |> 
  count(decade) |> 
  ggplot(aes(x = decade, y = n)) +
  geom_col(fill = "steelblue") +
  labs(title = "Number of Songs per Decade",
       x = "Decade",
       y = "Number of Songs")

```

```{r}
lyrics_words <- df |> 
  unnest_tokens(word, lyrics) |> 
  anti_join(stop_words) |> 
  rename(song_id = "...1")
```

```{r}
#general count
lyrics_words %>%
  count(decade, word, sort = TRUE) %>%
  group_by(decade) %>%
  slice_max(n, n = 10) %>%
  ggplot(aes(reorder_within(word, n, decade), n, fill = decade)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~decade, scales = "free") +
  scale_x_reordered() +
  coord_flip()
```
are the songs longer nowadays?
```{r}
lyrics_stats <- lyrics_words %>%
  group_by(song_id, decade) %>%
  summarise(total_words = n()) %>%
  group_by(decade) %>%
  summarise(mean_length = mean(total_words))

ggplot(lyrics_stats, aes(x = decade, y = mean_length)) +
  geom_line() +
  geom_point() +
  labs(title = "Average lyric length by decade")
```


# Sentimental Analysis

```{r}
sentiment_by_decade <- lyrics_words |> 
  inner_join(get_sentiments("bing")) |> 
  group_by(decade, sentiment) %>%
  count() |> 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |> 
  mutate(sentiment_score = positive - negative)

# Plot
ggplot(sentiment_by_decade, aes(x = decade, y = sentiment_score)) +
  geom_col(fill = "darkgreen") +
  labs(title = "Net Sentiment per Decade",
       y = "Sentiment Score (pos - neg)")

```

```{r}
nrc_emotions <- lyrics_words |> 
  inner_join(get_sentiments("nrc")) |> 
  count(decade, sentiment) |> 
  filter(sentiment %in% c("anger", "joy", "fear", "trust", "sadness"))

# Plot
ggplot(nrc_emotions, aes(x = decade, y = n, fill = sentiment)) +
  geom_col(position = "dodge") +
  labs(title = "Emotions per Decade (nrc)",
       y = "Count",
       x = "Decade")


```

```{r}
nrc_joy <- get_sentiments("nrc") |> 
  filter(sentiment == "joy")

joy_words_by_decade <- lyrics_words %>%
  inner_join(nrc_joy, by = "word") %>%
  count(decade, word, sort = TRUE)

joy_words_by_decade |> 
  group_by(decade) |> 
  top_n(10, n) |>  
  ungroup() |> 
  ggplot(aes(x = reorder_within(word, n, decade), y = n, fill = factor(decade))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ decade, scales = "free_y") +
  scale_x_reordered() +
  coord_flip() +
  labs(title = "Words associated with 'joy' by decade",
     x = "Word",
     y = "Frequency")
```

```{r}
nrc_anger <- get_sentiments("nrc") |> 
  filter(sentiment == "anger")

anger_words_by_decade <- lyrics_words %>%
  inner_join(nrc_anger, by = "word") %>%
  count(decade, word, sort = TRUE)

anger_words_by_decade |> 
  group_by(decade) |> 
  top_n(10, n) |>  
  ungroup() |> 
  ggplot(aes(x = reorder_within(word, n, decade), y = n, fill = factor(decade))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ decade, scales = "free_y") +
  scale_x_reordered() +
  coord_flip() +
  labs(title = "Words associated with 'anger' by decade",
     x = "Word",
     y = "Frequency")
```

```{r}
nrc_sadness <- get_sentiments("nrc") |> 
  filter(sentiment == "sadness")

sadness_words_by_decade <- lyrics_words %>%
  inner_join(nrc_sadness, by = "word") %>%
  count(decade, word, sort = TRUE)

sadness_words_by_decade |> 
  group_by(decade) |> 
  top_n(10, n) |>  
  ungroup() |> 
  ggplot(aes(x = reorder_within(word, n, decade), y = n, fill = factor(decade))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ decade, scales = "free_y") +
  scale_x_reordered() +
  coord_flip() +
  labs(title = "Words associated with 'sadness' by decade",
     x = "Word",
     y = "Frequency")
```

```{r}
bing_ratio <- lyrics_words %>%
  inner_join(get_sentiments("bing")) %>%
  count(decade, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 1) %>%
  mutate(ratio = positive / negative)

ggplot(bing_ratio, aes(x = decade, y = ratio)) +
  geom_line() +
  geom_point() +
  labs(title = "Positive-to-Negative Ratio per Decade",
       y = "Ratio (Positive / Negative)")
```

```{r}
library(wordcloud)
#general
lyrics_words %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  with(wordcloud(word, n, max.words = 100, random.order = FALSE, colors = c("red", "green")))
```

```{r}
#by decade
sentiment_by_decade <- lyrics_words %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(decade, word, sentiment, sort = TRUE)

split_data <- split(sentiment_by_decade, sentiment_by_decade$decade)


for (dec in names(split_data)) {
  data <- split_data[[dec]]
  

  dev.new(width = 8, height = 6)  
  
  with(data,
       wordcloud(words = word,
                 freq = n,
                 max.words = 100,
                 random.order = FALSE,
                 colors = ifelse(sentiment == "positive", "green", "red"))
  )
  title(main = paste("Wordcloud – Decade", dec), line = -1)
}

```

# Term Frequency

```{r}
total_words <- lyrics_words %>% 
  group_by(decade) %>% 
  summarize(total = sum(n()))

total_words
```

```{r}
song_words <- left_join(lyrics_words, total_words)
```

```{r}
song_words <- song_words %>%
  mutate(term_frequency = n()/total)
```

```{r}
ggplot(song_words, aes(term_frequency)) +
  geom_histogram(show.legend = TRUE) 
```

```{r}
freq_by_rank <- song_words |> 
  group_by(decade) |>  
  mutate(rank = row_number()) %>%
  ungroup()
```

```{r}
freq_by_rank |> 
  ggplot(aes(rank, term_frequency, color = decade)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

lol!!!!!!!!!!

# N-GRAMS

```{r}
songs_bigrams <- df |> 
  unnest_tokens(bigram, lyrics, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))

songs_bigrams
```

We can use count to examine the most frequent bigrams:

```{r}
songs_bigrams |> 
  count(bigram, sort = TRUE)
```

take into account: a lot of repeated words (???) ----\> no se yo que hariais pero yo alomejor intentaria eliminar el hecho de que dos palabras salieran unidas no se

Stopwords:

```{r}
bigrams_separated <- songs_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_separated
```

```{r}
bigrams_filtered <- bigrams_separated |> 
  filter(!word1 %in% stop_words$word) |> 
  filter(!word2 %in% stop_words$word)

bigrams_filtered
```

```{r}
bigrams_united <- bigrams_filtered 
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```

## Analysis

```{r}
bigram_counts_decade <- bigrams_united %>% 
  count(decade, bigram, sort = TRUE)

bigram_counts_decade
```

Let's see what is the most repeated word after love in these songs through the decades:

```{r}
bigrams_filtered %>%
  filter(word1 == "love") %>%
  count(decade, word1, word2, sort = TRUE)
```

```{r}
bigrams_filtered %>%
  filter(word1 == "hate") %>%
  count(decade, word1, word2, sort = TRUE)
```

```{r}
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)
```

Let's now see how much these specific bigrams appear through the decades -\> "love you", "feel good", "get high"

```{r}
#como alguna de estas combinaciones tienen stopwords volvemos a hacer el count que incluye las stop words

bigram_counts_decade <- songs_bigrams %>% 
  count(decade, bigram, sort = TRUE)

selected_bigrams <- c("broken heart", "kiss me", "feel good", #love themes
                      "get high", "all night", "dance all", #party themes
                      "kill you", "die young", "pull trigger", #violence
                      "black lives", "stand up", "the system") #social conscience 

bigram_counts_decade %>%
  filter(bigram %in% selected_bigrams)
```

```{r}
#not sure if this is useful
bigram_tf_idf <- bigrams_united %>%
  count(decade, bigram) %>%
  bind_tf_idf(bigram, decade, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```

-   Use `pairwise_count` to create a dataframe with word pairs co-occurring most often across the script.

```{r}
library(widyr)

word_pairs <- lyrics_words  |> 
  pairwise_count(word, song_id, sort = TRUE) 

word_pairs_by_decade <- lyrics_words |> 
  group_by(decade) |> 
  group_split() |> 
  map_df(~pairwise_count(.x, word, song_id, sort = TRUE) %>%
           mutate(decade = unique(.x$decade)))

```
```{r}
top_bigrams <- word_pairs_by_decade %>%
  filter(n >= 10) %>%
  group_by(decade) %>%
  slice_max(order_by = n, n = 20) %>%
  ungroup() %>%
  mutate(bigram = paste(item1, item2))  # Crear columna de texto combinada

# Ordenar por frecuencia dentro de cada faceta
top_bigrams <- top_bigrams %>%
  group_by(decade) %>%
  mutate(bigram = reorder_within(bigram, n, decade)) %>%
  ungroup()

# Gráfico de barras facetado
ggplot(top_bigrams, aes(bigram, n, fill = decade)) +
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~decade, scales = "free_y") +
  theme_minimal(base_size = 12) +
  labs(
    title = "Top 20 Word Pairs by Decade",
    x = "Bigram (word pair)",
    y = "Co-occurrence count"
  )
```



Use `pairwise_cors` to find correlations between words instead of just co-appearances.

```{r}
word_cors <- lyrics_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, song_id, sort = TRUE)

word_cors
```

-   Create a graph with word correlations using `ggraph`.

```{r}
set.seed(2016)

word_cors %>%
  filter(correlation > 0.4) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```
```{r}
word_cors_by_decade <- lyrics_words %>%
  group_by(decade) %>%
  group_split() %>%
  map_df(~ .x %>%
           group_by(word) %>%
           filter(n() >= 20) %>%
           pairwise_cor(word, song_id, sort = TRUE) %>%
           mutate(decade = unique(.x$decade))
  )

word_cors_by_decade %>%
  filter(decade == 2010, correlation > 0.4) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void() +
  ggtitle("Word Correlation Network – 2010s")

```
```{r}
word_cors_by_decade %>%
  filter(decade == 1950, correlation > 0.4) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void() +
  ggtitle("Word Correlation Network – 1950s")
```


# 

```{r}
word_counts <- df |> 
  unnest_tokens(word, lyrics) |> 
  anti_join(stop_words) |> 
  count(decade, word, sort = TRUE)

dtm <- word_counts |> 
  cast_dtm(decade, word, n)
```

```{r}
# LDA anwenden (z.B. 4 Themen, kannst du anpassen)
lda_model <- LDA(dtm, k = 4, control = list(seed = 1234))
```

```{r}
library(ggplot2)
library(tidytext)

# Beta-Tabelle: Wahrscheinlichkeit eines Wortes pro Topic
lda_topics <- tidy(lda_model)

# Top-Wörter je Topic
top_terms <- lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Plot
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  labs(
    title = "Top Words per Topic (LDA)",
    x = "Beta (Word Importance in Topic)",
    y = NULL
  )


```

```{r}


# 1. Wahrscheinlichkeiten (gamma) für jedes Dokument (Song) extrahieren
gamma_df <- tidy(lda_model, matrix = "gamma")  # lda_model = dein LDA-Modell

# 2. release_date hinzufügen (damit wir Jahrzehnte analysieren können)
# Angenommen deine tidy DTM heißt `lyrics_dtm`
# und du hast eine Spalte `release_date` im Original-Dataframe df

doc_info <- df %>%
  mutate(document = row_number(),              # muss zur Reihenfolge im DTM passen!
         decade = floor(release_date / 10) * 10) %>%
  select(document, decade)

# 3. Gamma-Werte mit Jahrzehnten verbinden
gamma_decade <- gamma_df %>%
  mutate(document = as.integer(document)) %>%
  left_join(doc_info, by = "document")

# 4. Durchschnittliche Themenverteilung pro Dekade berechnen
topic_by_decade <- gamma_decade %>%
  group_by(decade, topic) %>%
  summarize(mean_gamma = mean(gamma), .groups = "drop")

# 5. Plot: Thema-Verteilung pro Dekade
ggplot(topic_by_decade, aes(x = decade, y = mean_gamma, fill = factor(topic))) +
  geom_col(position = "dodge") +
  labs(title = "Topic Prevalence per Decade",
       x = "Decade",
       y = "Average Gamma (Topic Proportion)",
       fill = "Topic") +
  theme_minimal()

```

```{r}
library(tidytext)
library(dplyr)

df <- df %>%
  mutate(song_id = row_number(),
         decade = floor(as.numeric(release_date) / 10) * 10)

# Tokenisierung + Stopwords entfernen
lyrics_words <- df %>%
  unnest_tokens(word, lyrics) %>%
  anti_join(stop_words)

# DTM mit einem Dokument pro Song
lyrics_dtm <- lyrics_words %>%
  count(song_id, word) %>%
  cast_dtm(song_id, word, n)



```

```{r}
library(topicmodels)
lyrics_lda <- LDA(lyrics_dtm, k = 4, control = list(seed = 1234))

```

```{r}
gamma_df <- tidy(lyrics_lda, matrix = "gamma") %>%
  mutate(song_id = as.integer(document)) %>%
  left_join(df %>% select(song_id, decade), by = "song_id")

```

```{r}
gamma_df %>%
  group_by(decade, topic) %>%
  summarise(avg_gamma = mean(gamma)) %>%
  ggplot(aes(x = decade, y = avg_gamma, fill = as.factor(topic))) +
  geom_col(position = "dodge") +
  labs(title = "Topic Prevalence per Decade",
       x = "Decade", y = "Average Gamma (Topic Proportion)", fill = "Topic")


```
